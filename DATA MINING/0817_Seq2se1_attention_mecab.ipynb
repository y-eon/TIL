{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"nmt_with_attention_mecab.ipynb","provenance":[{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb","timestamp":1597474841193}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s_qNSzzyaCbD"},"source":["##### Copyright 2019 The TensorFlow Authors."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"J0Qjg6vuaHNt"},"source":["# Neural machine translation with attention"]},{"cell_type":"code","metadata":{"id":"0Z8HH4v8CVXr","colab_type":"code","colab":{}},"source":["# # Mecab 설치\n","# !sudo apt-get install g++ openjdk-7-jdk # Install Java 1.7+\n","# !sudo apt-get install python-dev; pip install konlpy     # Python 2.x\n","# !sudo apt-get install python3-dev; pip3 install konlpy   # Python 3.x\n","# !sudo apt-get install curl\n","# !bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n","from konlpy.tag import Mecab\n","mecab = Mecab()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zri7xVrRBZ_g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1597591169516,"user_tz":-540,"elapsed":19697,"user":{"displayName":"김도연","photoUrl":"","userId":"13093336926317411579"}},"outputId":"bb625a65-5ef0-4066-ead5-3a71b2bb6343"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tnxXKDjq3jEL","colab":{}},"source":["import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmuTHd2vBjSZ","colab_type":"code","colab":{}},"source":["import pandas as pd\n","df= pd.read_excel('/content/gdrive/My Drive/Seq2Seq/data/kor.xlsx',sheet_name=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVeZI-EZBoTm","colab_type":"code","colab":{}},"source":["# Download the file\n","path_to_file = '/content/gdrive/My Drive/Seq2Seq/data/kor.xlsx'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rd0jw-eC3jEh","colab":{}},"source":["# Converts the unicode file to ascii\n","def unicode_to_ascii(s):\n","  return ''.join(c for c in unicodedata.normalize('NFD', s)\n","      if unicodedata.category(c) != 'Mn')\n","\n","\n","def preprocess_sentence(w):\n","  w = w.lower().strip()\n","\n","  # creating a space between a word and the punctuation following it\n","  # eg: \"he is a boy.\" => \"he is a boy .\"\n","  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n","  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","  w = re.sub(r'[\" \"]+', \" \", w)\n","  \n","  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","  w = re.sub(r\"[^ㄱ-ㅣ가-힣a-zA-Z?.!,]+\", \" \", w)\n","  # w = re.sub(r\"[^ ㄱ-ㅣ가-힣?.!,¿]+\", \" \", w)\n","  w = w.strip()\n","\n","  # adding a start and an end token to the sentence\n","  # so that the model know when to start and stop predicting.\n","  # w = '<start> ' + w + ' <end>'\n","  return w"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"opI2GzOt479E","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597591190520,"user_tz":-540,"elapsed":1212,"user":{"displayName":"김도연","photoUrl":"","userId":"13093336926317411579"}},"outputId":"2b1469cb-e7e1-4758-eb03-30d898d98761"},"source":["ko_sentence = '나는 매일 저녁 배트를 만나러 다락방으로 가요.'\n","en_sentence = \"I go to the attic every evening to meet Bat.\"\n","print(preprocess_sentence(en_sentence))\n","print(preprocess_sentence(ko_sentence))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["i go to the attic every evening to meet bat .\n","나는 매일 저녁 배트를 만나러 다락방으로 가요 .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OHn4Dct23jEm","colab":{}},"source":["# 1. Remove the accents\n","# 2. Clean the sentences\n","# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n","def create_dataset(path, num_examples):\n","  df = pd.read_excel(path,sheet_name=1)\n","  col = df.columns\n","  lines = df[col[1]] +'\\t'+ df[col[2]]\n","  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n","\n","  return zip(*word_pairs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cTbSbBz55QtF","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597564672868,"user_tz":-540,"elapsed":20539,"user":{"displayName":"김도연","photoUrl":"","userId":"13093336926317411579"}},"outputId":"e4484d4e-6b00-476b-b410-78a81cce39f4"},"source":["# en, sp = create_dataset(path_to_file, None)\n","# print(en[:2])\n","# print(sp[-1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('나는 매일 저녁 배트를 만나러 다락방으로 가요 .', '선생님 이문장이 이해가 안 가요 .')\n","it is difficult for young children to enjoy sports .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bIOn8RCNDJXG","colab":{}},"source":["def tokenize_en(lang):\n","  lang = list(lang)\n","  for i in range(len(lang)):\n","    lang[i] = '<start> ' + lang[i] + ' <end>'\n","\n","  lang = tuple(lang)\n","  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","      filters='')\n","  lang_tokenizer.fit_on_texts(lang)\n","\n","  tensor = lang_tokenizer.texts_to_sequences(lang)\n","\n","  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n","                                                         padding='post')\n","\n","  return tensor, lang_tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fe1MI7_OBGBG","colab_type":"code","colab":{}},"source":["def tokenize_ko(lang):\n","  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>',\n","      filters='')\n","  lang_mecab = []\n","  for i in range(len(lang)):\n","      sentence = [token[0] for token in mecab.pos(lang[i])]\n","      tmp = '<start>'\n","      for word in sentence:\n","        tmp += ' ' + word\n","        # tmp += word\n","      tmp += ' <end>'\n","      lang_mecab.append(tmp)\n","  lang_mecab = tuple(lang_mecab)\n","\n","  lang_tokenizer.fit_on_texts(lang_mecab)\n","\n","  tensor = lang_tokenizer.texts_to_sequences(lang_mecab)\n","\n","  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n","                                                         padding='post')\n","\n","  return tensor, lang_tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eAY9k49G3jE_","colab":{}},"source":["def load_dataset(path, num_examples=None):\n","  # creating cleaned input, output pairs\n","  inp_lang, targ_lang = create_dataset(path, num_examples)\n","\n","  input_tensor, inp_lang_tokenizer = tokenize_ko(inp_lang)\n","  target_tensor, targ_lang_tokenizer = tokenize_en(targ_lang)\n","\n","  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cnxC7q-j3jFD","colab":{}},"source":["# Try experimenting with the size of that dataset\n","num_examples = 75000\n","input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n","\n","# Calculate max_length of the target tensors\n","max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pIwDwlmpGjpa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597591608321,"user_tz":-540,"elapsed":12764,"user":{"displayName":"김도연","photoUrl":"","userId":"13093336926317411579"}},"outputId":"534c6881-6625-421a-9c95-f0ed0389e027"},"source":["input_tensor.shape, target_tensor.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((75000, 32), (75000, 21))"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4QILQkOs3jFG","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597591611257,"user_tz":-540,"elapsed":1036,"user":{"displayName":"김도연","photoUrl":"","userId":"13093336926317411579"}},"outputId":"59c47d16-1336-4c54-c1b5-ccdf7c92d0ee"},"source":["# Creating training and validation sets using an 80-20 split\n","input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=42)\n","\n","# Show length\n","print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["60000 60000 15000 15000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lJPmLZGMeD5q","colab":{}},"source":["def convert(lang, tensor):\n","  for t in tensor:\n","    if t!=0:\n","      print (\"%d ----> %s\" % (t, lang.index_word[t]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VXukARTDd7MT","colab":{"base_uri":"https://localhost:8080/","height":699},"executionInfo":{"status":"ok","timestamp":1597591615699,"user_tz":-540,"elapsed":619,"user":{"displayName":"김도연","photoUrl":"","userId":"13093336926317411579"}},"outputId":"ae1d393e-84cf-45d6-f6f0-9dabfb98dc7f"},"source":["print (\"Input Language; index to word mapping\")\n","convert(inp_lang, input_tensor_train[0])\n","print ()\n","print (\"Target Language; index to word mapping\")\n","convert(targ_lang, target_tensor_train[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input Language; index to word mapping\n","2 ----> <start>\n","55 ----> 일\n","535 ----> 주일\n","258 ----> 동안\n","40 ----> 우리\n","1424 ----> 게스트\n","1345 ----> 하우스\n","8 ----> 에\n","11 ----> 있\n","39 ----> 었\n","99 ----> 지만\n","128 ----> 너무\n","1881 ----> 금방\n","81 ----> 시간\n","5 ----> 이\n","9650 ----> 흘러간\n","21 ----> 것\n","59 ----> 같\n","41 ----> 아\n","4 ----> .\n","3 ----> <end>\n","\n","Target Language; index to word mapping\n","1 ----> <start>\n","5 ----> the\n","138 ----> week\n","7 ----> you\n","1216 ----> stayed\n","12 ----> in\n","16 ----> my\n","1112 ----> guest\n","155 ----> house\n","74 ----> has\n","977 ----> gone\n","139 ----> too\n","660 ----> fast\n","3 ----> .\n","2 ----> <end>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rgCLkfv5uO3d"},"source":["### Create a tf.data dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TqHsArVZ3jFS","colab":{}},"source":["BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 64\n","steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n","embedding_dim = 256\n","units = 256\n","vocab_inp_size = len(inp_lang.word_index)+1\n","vocab_tar_size = len(targ_lang.word_index)+1\n","\n","dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qc6-NK1GtWQt","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597591640333,"user_tz":-540,"elapsed":5809,"user":{"displayName":"김도연","photoUrl":"","userId":"13093336926317411579"}},"outputId":"048ce2f2-be4c-4830-ede8-4beb664ecf9b"},"source":["example_input_batch, example_target_batch = next(iter(dataset))\n","example_input_batch.shape, example_target_batch.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([64, 32]), TensorShape([64, 21]))"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nZ2rI24i3jFg","colab":{}},"source":["class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","    super(Encoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.enc_units = enc_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.enc_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","\n","  def call(self, x, hidden):\n","    x = self.embedding(x)\n","    output, state = self.gru(x, initial_state = hidden)\n","    return output, state\n","\n","  def initialize_hidden_state(self):\n","    return tf.zeros((self.batch_sz, self.enc_units))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"60gSVh05Jl6l","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597591647037,"user_tz":-540,"elapsed":5806,"user":{"displayName":"김도연","photoUrl":"","userId":"13093336926317411579"}},"outputId":"3addb96e-c0da-4dcf-e7c7-a086fd8611a3"},"source":["encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","\n","# sample input\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n","print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n","print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Encoder output shape: (batch size, sequence length, units) (64, 32, 256)\n","Encoder Hidden state shape: (batch size, units) (64, 256)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"umohpBN2OM94","colab":{}},"source":["class BahdanauAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, query, values):\n","    # query hidden state shape == (batch_size, hidden size)\n","    # query_with_time_axis shape == (batch_size, 1, hidden size)\n","    # values shape == (batch_size, max_len, hidden size)\n","    # we are doing this to broadcast addition along the time axis to calculate the score\n","    query_with_time_axis = tf.expand_dims(query, 1)\n","\n","    # score shape == (batch_size, max_length, 1)\n","    # we get 1 at the last axis because we are applying score to self.V\n","    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","    score = self.V(tf.nn.tanh(\n","        self.W1(query_with_time_axis) + self.W2(values)))\n","\n","    # attention_weights shape == (batch_size, max_length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * values\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"k534zTHiDjQU","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597591648868,"user_tz":-540,"elapsed":5482,"user":{"displayName":"김도연","photoUrl":"","userId":"13093336926317411579"}},"outputId":"1cbefb60-8aa5-46fd-b81a-bbdf9db8c7cd"},"source":["attention_layer = BahdanauAttention(10)\n","attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n","\n","print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n","print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Attention result shape: (batch size, units) (64, 256)\n","Attention weights shape: (batch_size, sequence_length, 1) (64, 32, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yJ_B3mhW3jFk","colab":{}},"source":["class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","    super(Decoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.dec_units = dec_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.dec_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","    # used for attention\n","    self.attention = BahdanauAttention(self.dec_units)\n","\n","  def call(self, x, hidden, enc_output):\n","    # enc_output shape == (batch_size, max_length, hidden_size)\n","    context_vector, attention_weights = self.attention(hidden, enc_output)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # passing the concatenated vector to the GRU\n","    output, state = self.gru(x)\n","\n","    # output shape == (batch_size * 1, hidden_size)\n","    output = tf.reshape(output, (-1, output.shape[2]))\n","\n","    # output shape == (batch_size, vocab)\n","    x = self.fc(output)\n","\n","    return x, state, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"P5UY8wko3jFp","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597591655929,"user_tz":-540,"elapsed":1050,"user":{"displayName":"김도연","photoUrl":"","userId":"13093336926317411579"}},"outputId":"cc4a7ff2-8763-414e-db89-117fb4c753c9"},"source":["decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n","\n","sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n","                                      sample_hidden, sample_output)\n","\n","print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Decoder output shape: (batch_size, vocab size) (64, 19231)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_ch_71VbIRfK"},"source":["## Define the optimizer and the loss function"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WmTHr5iV3jFr","colab":{}},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DMVWzzsfNl4e"},"source":["## Checkpoints (Object-based saving)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Zj8bXQTgNwrF","colab":{}},"source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hpObfY22IddU"},"source":["## Training\n","\n","1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n","2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n","3. The decoder returns the *predictions* and the *decoder hidden state*.\n","4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n","5. Use *teacher forcing* to decide the next input to the decoder.\n","6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n","7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sC9ArXSsVfqn","colab":{}},"source":["@tf.function\n","def train_step(inp, targ, enc_hidden):\n","  loss = 0\n","\n","  with tf.GradientTape() as tape:\n","    enc_output, enc_hidden = encoder(inp, enc_hidden)\n","\n","    dec_hidden = enc_hidden\n","\n","    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n","\n","    # Teacher forcing - feeding the target as the next input\n","    for t in range(1, targ.shape[1]):\n","      # passing enc_output to the decoder\n","      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","\n","      loss += loss_function(targ[:, t], predictions)\n","\n","      # using teacher forcing\n","      dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","  batch_loss = (loss / int(targ.shape[1]))\n","\n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","  gradients = tape.gradient(loss, variables)\n","\n","  optimizer.apply_gradients(zip(gradients, variables))\n","\n","  return batch_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ddefjBMa3jF0","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597593292266,"user_tz":-540,"elapsed":31,"user":{"displayName":"김도연","photoUrl":"","userId":"13093336926317411579"}},"outputId":"f44a748e-0066-49b4-8128-1b0f74d5ac20"},"source":["EPOCHS = 10\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  enc_hidden = encoder.initialize_hidden_state()\n","  total_loss = 0\n","\n","  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","    batch_loss = train_step(inp, targ, enc_hidden)\n","    total_loss += batch_loss\n","\n","    if batch % 100 == 0:\n","      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n","                                                   batch,\n","                                                   batch_loss.numpy()))\n","  # saving (checkpoint) the model every 2 epochs\n","  if (epoch + 1) % 2 == 0:\n","    checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n","                                      total_loss / steps_per_epoch))\n","  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 5.4313\n","Epoch 1 Batch 100 Loss 3.1931\n","Epoch 1 Batch 200 Loss 3.0080\n","Epoch 1 Batch 300 Loss 2.8521\n","Epoch 1 Batch 400 Loss 2.8100\n","Epoch 1 Batch 500 Loss 2.8070\n","Epoch 1 Batch 600 Loss 2.6044\n","Epoch 1 Batch 700 Loss 2.5684\n","Epoch 1 Batch 800 Loss 2.5561\n","Epoch 1 Batch 900 Loss 2.6634\n","Epoch 1 Loss 2.8637\n","Time taken for 1 epoch 141.77857637405396 sec\n","\n","Epoch 2 Batch 0 Loss 2.5140\n","Epoch 2 Batch 100 Loss 2.4107\n","Epoch 2 Batch 200 Loss 2.6330\n","Epoch 2 Batch 300 Loss 2.4372\n","Epoch 2 Batch 400 Loss 2.4259\n","Epoch 2 Batch 500 Loss 2.5146\n","Epoch 2 Batch 600 Loss 2.3246\n","Epoch 2 Batch 700 Loss 2.3839\n","Epoch 2 Batch 800 Loss 2.3225\n","Epoch 2 Batch 900 Loss 2.3438\n","Epoch 2 Loss 2.4421\n","Time taken for 1 epoch 120.85313200950623 sec\n","\n","Epoch 3 Batch 0 Loss 2.1406\n","Epoch 3 Batch 100 Loss 2.2848\n","Epoch 3 Batch 200 Loss 2.3403\n","Epoch 3 Batch 300 Loss 2.2194\n","Epoch 3 Batch 400 Loss 2.1669\n","Epoch 3 Batch 500 Loss 2.0888\n","Epoch 3 Batch 600 Loss 2.2815\n","Epoch 3 Batch 700 Loss 2.2752\n","Epoch 3 Batch 800 Loss 2.2007\n","Epoch 3 Batch 900 Loss 2.0200\n","Epoch 3 Loss 2.1990\n","Time taken for 1 epoch 120.45555019378662 sec\n","\n","Epoch 4 Batch 0 Loss 1.9784\n","Epoch 4 Batch 100 Loss 1.9582\n","Epoch 4 Batch 200 Loss 2.0806\n","Epoch 4 Batch 300 Loss 1.9321\n","Epoch 4 Batch 400 Loss 2.0256\n","Epoch 4 Batch 500 Loss 2.0396\n","Epoch 4 Batch 600 Loss 1.8685\n","Epoch 4 Batch 700 Loss 2.0366\n","Epoch 4 Batch 800 Loss 1.8862\n","Epoch 4 Batch 900 Loss 1.9500\n","Epoch 4 Loss 1.9885\n","Time taken for 1 epoch 121.09216165542603 sec\n","\n","Epoch 5 Batch 0 Loss 1.7755\n","Epoch 5 Batch 100 Loss 1.7867\n","Epoch 5 Batch 200 Loss 1.7174\n","Epoch 5 Batch 300 Loss 1.7577\n","Epoch 5 Batch 400 Loss 1.8926\n","Epoch 5 Batch 500 Loss 1.8432\n","Epoch 5 Batch 600 Loss 1.8491\n","Epoch 5 Batch 700 Loss 1.8019\n","Epoch 5 Batch 800 Loss 1.7326\n","Epoch 5 Batch 900 Loss 1.7307\n","Epoch 5 Loss 1.8062\n","Time taken for 1 epoch 120.58811092376709 sec\n","\n","Epoch 6 Batch 0 Loss 1.6287\n","Epoch 6 Batch 100 Loss 1.6118\n","Epoch 6 Batch 200 Loss 1.6555\n","Epoch 6 Batch 300 Loss 1.7221\n","Epoch 6 Batch 400 Loss 1.6689\n","Epoch 6 Batch 500 Loss 1.5696\n","Epoch 6 Batch 600 Loss 1.6450\n","Epoch 6 Batch 700 Loss 1.6267\n","Epoch 6 Batch 800 Loss 1.7526\n","Epoch 6 Batch 900 Loss 1.5619\n","Epoch 6 Loss 1.6519\n","Time taken for 1 epoch 120.93777990341187 sec\n","\n","Epoch 7 Batch 0 Loss 1.4895\n","Epoch 7 Batch 100 Loss 1.5300\n","Epoch 7 Batch 200 Loss 1.5136\n","Epoch 7 Batch 300 Loss 1.5592\n","Epoch 7 Batch 400 Loss 1.5735\n","Epoch 7 Batch 500 Loss 1.5803\n","Epoch 7 Batch 600 Loss 1.4983\n","Epoch 7 Batch 700 Loss 1.5155\n","Epoch 7 Batch 800 Loss 1.4255\n","Epoch 7 Batch 900 Loss 1.5125\n","Epoch 7 Loss 1.5138\n","Time taken for 1 epoch 120.89300155639648 sec\n","\n","Epoch 8 Batch 0 Loss 1.3521\n","Epoch 8 Batch 100 Loss 1.3421\n","Epoch 8 Batch 200 Loss 1.4909\n","Epoch 8 Batch 300 Loss 1.3796\n","Epoch 8 Batch 400 Loss 1.2831\n","Epoch 8 Batch 500 Loss 1.3423\n","Epoch 8 Batch 600 Loss 1.4862\n","Epoch 8 Batch 700 Loss 1.2730\n","Epoch 8 Batch 800 Loss 1.3988\n","Epoch 8 Batch 900 Loss 1.3160\n","Epoch 8 Loss 1.3884\n","Time taken for 1 epoch 120.97288417816162 sec\n","\n","Epoch 9 Batch 0 Loss 1.3035\n","Epoch 9 Batch 100 Loss 1.2214\n","Epoch 9 Batch 200 Loss 1.2774\n","Epoch 9 Batch 300 Loss 1.2391\n","Epoch 9 Batch 400 Loss 1.2016\n","Epoch 9 Batch 500 Loss 1.2090\n","Epoch 9 Batch 600 Loss 1.2742\n","Epoch 9 Batch 700 Loss 1.2342\n","Epoch 9 Batch 800 Loss 1.3698\n","Epoch 9 Batch 900 Loss 1.3015\n","Epoch 9 Loss 1.2726\n","Time taken for 1 epoch 121.08213520050049 sec\n","\n","Epoch 10 Batch 0 Loss 1.0980\n","Epoch 10 Batch 100 Loss 1.0856\n","Epoch 10 Batch 200 Loss 1.2400\n","Epoch 10 Batch 300 Loss 1.0916\n","Epoch 10 Batch 400 Loss 1.1313\n","Epoch 10 Batch 500 Loss 1.2372\n","Epoch 10 Batch 600 Loss 1.1228\n","Epoch 10 Batch 700 Loss 1.1511\n","Epoch 10 Batch 800 Loss 1.1215\n","Epoch 10 Batch 900 Loss 1.1934\n","Epoch 10 Loss 1.1672\n","Time taken for 1 epoch 120.76000642776489 sec\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mU3Ce8M6I3rz"},"source":["## Translate\n","\n","* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n","* Stop predicting when the model predicts the *end token*.\n","* And store the *attention weights for every time step*.\n","\n","Note: The encoder output is calculated only once for one input."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EbQpyYs13jF_","colab":{}},"source":["def evaluate(sentence):\n","  attention_plot = np.zeros((max_length_targ, max_length_inp))\n","\n","  sentence = preprocess_sentence(sentence)\n","  sentence_ = [token[0] for token in mecab.pos(sentence)]\n","  tmp = '<start>'\n","  for word in sentence_:\n","      tmp += ' ' + word\n","        # tmp += word\n","  tmp += ' <end>'\n","  \n","  inputs = [inp_lang.word_index[i] for i in tmp.split(' ')]\n","  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                         maxlen=max_length_inp,\n","                                                         padding='post')\n","  inputs = tf.convert_to_tensor(inputs)\n","\n","  result = ''\n","\n","  hidden = [tf.zeros((1, units))]\n","  enc_out, enc_hidden = encoder(inputs, hidden)\n","\n","  dec_hidden = enc_hidden\n","  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n","\n","  for t in range(max_length_targ):\n","    predictions, dec_hidden, attention_weights = decoder(dec_input,\n","                                                         dec_hidden,\n","                                                         enc_out)\n","\n","    # storing the attention weights to plot later on\n","    attention_weights = tf.reshape(attention_weights, (-1, ))\n","    attention_plot[t] = attention_weights.numpy()\n","\n","    predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","    result += targ_lang.index_word[predicted_id] + ' '\n","\n","    if targ_lang.index_word[predicted_id] == '<end>':\n","      return result, sentence, attention_plot\n","\n","    # the predicted ID is fed back into the model\n","    dec_input = tf.expand_dims([predicted_id], 0)\n","\n","  return result, sentence, attention_plot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"s5hQWlbN3jGF","colab":{}},"source":["# function for plotting the attention weights\n","def plot_attention(attention, sentence, predicted_sentence):\n","  fig = plt.figure(figsize=(10,10))\n","  ax = fig.add_subplot(1, 1, 1)\n","  ax.matshow(attention, cmap='viridis')\n","\n","  fontdict = {'fontsize': 14}\n","\n","  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n","  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n","\n","  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sl9zUHzg3jGI","colab":{}},"source":["def translate(sentence):\n","  result, sentence, attention_plot = evaluate(sentence)\n","\n","  print('Input: %s' % (sentence))\n","  print('Predicted translation: {}'.format(result))\n","  \n","  # attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n","  # plot_attention(attention_plot, sentence.split(' '), result.split(' '))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n250XbnjOaqP"},"source":["## Restore the latest checkpoint and test"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UJpT9D5_OgP6","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597593864724,"user_tz":-540,"elapsed":1594,"user":{"displayName":"김도연","photoUrl":"","userId":"13093336926317411579"}},"outputId":"0b6f70c4-e19a-4f3e-841a-446cc87c0d4a"},"source":["# restoring the latest checkpoint in checkpoint_dir\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f7bcc3f8cf8>"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"code","metadata":{"id":"eW7nsqDZM42z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":197},"executionInfo":{"status":"ok","timestamp":1597593864725,"user_tz":-540,"elapsed":1416,"user":{"displayName":"김도연","photoUrl":"","userId":"13093336926317411579"}},"outputId":"280f9ef2-99b6-4a5b-a591-414804507972"},"source":["df[:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mid_sid</th>\n","      <th>ko</th>\n","      <th>en</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>나는 매일 저녁 배트를 만나러 다락방으로 가요.</td>\n","      <td>I go to the attic every evening to meet Bat.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>선생님 이문장이 이해가 안 가요.</td>\n","      <td>Sir, I don't understand this sentence here.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>컴퓨터를 시작하면 시간이 너무 빠르게 가요.</td>\n","      <td>Time flies when you start using the computer.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>나는 오늘 자정에 한국으로 돌아 가요.</td>\n","      <td>I'm going back to Korea today at midnight.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>나는 일어나자마자 화장실에 가요.</td>\n","      <td>I go to bathroom as soon as I wake up.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   mid_sid  ...                                             en\n","0        1  ...   I go to the attic every evening to meet Bat.\n","1        2  ...    Sir, I don't understand this sentence here.\n","2        3  ...  Time flies when you start using the computer.\n","3        4  ...     I'm going back to Korea today at midnight.\n","4        5  ...         I go to bathroom as soon as I wake up.\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zSx2iM36EZQZ","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597593883351,"user_tz":-540,"elapsed":1639,"user":{"displayName":"김도연","photoUrl":"","userId":"13093336926317411579"}},"outputId":"97001088-5312-4e0d-c955-c1e5f0cf0e8a"},"source":["translate('컴퓨터를 시작하면 시간이 너무 빠르게 가요.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input: 컴퓨터를 시작하면 시간이 너무 빠르게 가요 .\n","Predicted translation: it is time when you have a rest of the room is a long time . <end> \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A3LLCx3ZE0Ls","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597593865112,"user_tz":-540,"elapsed":1441,"user":{"displayName":"김도연","photoUrl":"","userId":"13093336926317411579"}},"outputId":"6c33944f-d5f7-4f5a-9f90-8fc5360ca174"},"source":["translate('나는 일어나자마자 화장실에 가요.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input: 나는 일어나자마자 화장실에 가요 .\n","Predicted translation: i go to the subway as i go to bed . <end> \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1LZbTbVIDXKW","colab_type":"text"},"source":["## BLEU SCORE"]}]}